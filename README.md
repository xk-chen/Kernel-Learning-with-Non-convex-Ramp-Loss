# Kernel-Learning-with-Non-convex-Ramp-Loss

We study the kernel learning problems with ramp loss, a non-convex but noise-resistant loss function. In this work, we justify the validity of ramp loss under classical kernel learning framework, in particular, we show that the generalization bound for empirical ramp risk minimizer is similar to that of convex surrogate losses, which implies kernel learning with such loss function is not only noise resistant but, more important, statistically consistent. For adapting to large-scale and real-time scenarios, we introduce PA-ramp, a heuristic online algorithm based on passive aggressive framework, to solve this learning problem. Empirically, with less support vectors, this algorithm achieves comparable empirical performances to varied online methods.
